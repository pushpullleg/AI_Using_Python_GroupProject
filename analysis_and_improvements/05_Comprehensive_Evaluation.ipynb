{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05. Comprehensive Evaluation\n",
        "\n",
        "## Objective\n",
        "Create a comprehensive evaluation comparing:\n",
        "1. Baseline models vs ML models\n",
        "2. Different balancing techniques\n",
        "3. Feature importance insights\n",
        "4. Overall model performance assessment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score, classification_report, \n",
        "                           confusion_matrix, roc_auc_score, \n",
        "                           precision_recall_fscore_support)\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "\n",
        "print(\"‚úÖ Libraries imported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data and Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = pd.read_csv('../today/trajectory_ml_ready_advanced.csv')\n",
        "drop_cols = ['UNITID', 'Institution_Name', 'Year', 'Target_Trajectory', 'Target_Label', 'State']\n",
        "X = df.drop(columns=drop_cols)\n",
        "y = df['Target_Label'].astype(int)\n",
        "X = pd.get_dummies(X, columns=['Division'], drop_first=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Load saved model\n",
        "model = joblib.load('../today/models/final_trajectory_model.joblib')\n",
        "\n",
        "print(f\"Data loaded: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "print(f\"Train: {X_train.shape[0]}, Test: {X_test.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Evaluate All Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions from saved model\n",
        "y_pred_ml = model.predict(X_test)\n",
        "y_prob_ml = model.predict_proba(X_test)\n",
        "\n",
        "# Calculate comprehensive metrics\n",
        "def calculate_metrics(y_true, y_pred, y_prob=None, model_name=\"\"):\n",
        "    \"\"\"Calculate comprehensive metrics\"\"\"\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy_score(y_true, y_pred)\n",
        "    }\n",
        "    \n",
        "    if y_prob is not None:\n",
        "        try:\n",
        "            metrics['ROC-AUC'] = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
        "        except:\n",
        "            metrics['ROC-AUC'] = None\n",
        "    \n",
        "    # Per-class metrics\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=[0, 1, 2], zero_division=0\n",
        "    )\n",
        "    \n",
        "    metrics['Declining_Precision'] = precision[0]\n",
        "    metrics['Declining_Recall'] = recall[0]\n",
        "    metrics['Declining_F1'] = f1[0]\n",
        "    \n",
        "    metrics['Stable_Precision'] = precision[1]\n",
        "    metrics['Stable_Recall'] = recall[1]\n",
        "    metrics['Stable_F1'] = f1[1]\n",
        "    \n",
        "    metrics['Improving_Precision'] = precision[2]\n",
        "    metrics['Improving_Recall'] = recall[2]\n",
        "    metrics['Improving_F1'] = f1[2]\n",
        "    \n",
        "    # Macro averages\n",
        "    metrics['Macro_Precision'] = precision.mean()\n",
        "    metrics['Macro_Recall'] = recall.mean()\n",
        "    metrics['Macro_F1'] = f1.mean()\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Evaluate ML model\n",
        "ml_metrics = calculate_metrics(y_test, y_pred_ml, y_prob_ml, \"XGBoost (ML)\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ML MODEL PERFORMANCE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nOverall Metrics:\")\n",
        "print(f\"  Accuracy: {ml_metrics['Accuracy']:.4f}\")\n",
        "print(f\"  ROC-AUC: {ml_metrics['ROC-AUC']:.4f}\")\n",
        "print(f\"  Macro F1: {ml_metrics['Macro_F1']:.4f}\")\n",
        "\n",
        "print(f\"\\nPer-Class Performance:\")\n",
        "print(f\"  Declining - Precision: {ml_metrics['Declining_Precision']:.4f}, \"\n",
        "      f\"Recall: {ml_metrics['Declining_Recall']:.4f}, F1: {ml_metrics['Declining_F1']:.4f}\")\n",
        "print(f\"  Stable - Precision: {ml_metrics['Stable_Precision']:.4f}, \"\n",
        "      f\"Recall: {ml_metrics['Stable_Recall']:.4f}, F1: {ml_metrics['Stable_F1']:.4f}\")\n",
        "print(f\"  Improving - Precision: {ml_metrics['Improving_Precision']:.4f}, \"\n",
        "      f\"Recall: {ml_metrics['Improving_Recall']:.4f}, F1: {ml_metrics['Improving_F1']:.4f}\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_ml)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Declining', 'Stable', 'Improving'],\n",
        "            yticklabels=['Declining', 'Stable', 'Improving'])\n",
        "plt.title('Confusion Matrix - XGBoost Model')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix_ml.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Compare with Baselines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple baselines\n",
        "majority_class = y_train.mode()[0]\n",
        "y_pred_majority = np.full(len(y_test), majority_class)\n",
        "majority_metrics = calculate_metrics(y_test, y_pred_majority, None, \"Majority Class\")\n",
        "\n",
        "# Random guess\n",
        "np.random.seed(42)\n",
        "y_pred_random = np.random.choice([0, 1, 2], size=len(y_test))\n",
        "random_metrics = calculate_metrics(y_test, y_pred_random, None, \"Random Guess\")\n",
        "\n",
        "# Combine all results\n",
        "all_metrics = pd.DataFrame([ml_metrics, majority_metrics, random_metrics])\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(all_metrics[['Model', 'Accuracy', 'ROC-AUC', 'Macro_F1', \n",
        "                   'Declining_F1', 'Stable_F1', 'Improving_F1']].to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Accuracy\n",
        "axes[0, 0].bar(all_metrics['Model'], all_metrics['Accuracy'], color=['#9b59b6', '#3498db', '#e74c3c'])\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].set_title('Accuracy Comparison')\n",
        "axes[0, 0].set_ylim(0, 1)\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Macro F1\n",
        "axes[0, 1].bar(all_metrics['Model'], all_metrics['Macro_F1'], color=['#9b59b6', '#3498db', '#e74c3c'])\n",
        "axes[0, 1].set_ylabel('Macro F1-Score')\n",
        "axes[0, 1].set_title('Macro F1-Score Comparison')\n",
        "axes[0, 1].set_ylim(0, 1)\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Per-class F1\n",
        "x = np.arange(len(all_metrics))\n",
        "width = 0.25\n",
        "axes[1, 0].bar(x - width, all_metrics['Declining_F1'], width, label='Declining', color='#e74c3c')\n",
        "axes[1, 0].bar(x, all_metrics['Stable_F1'], width, label='Stable', color='#3498db')\n",
        "axes[1, 0].bar(x + width, all_metrics['Improving_F1'], width, label='Improving', color='#2ecc71')\n",
        "axes[1, 0].set_ylabel('F1-Score')\n",
        "axes[1, 0].set_title('Per-Class F1-Score')\n",
        "axes[1, 0].set_xticks(x)\n",
        "axes[1, 0].set_xticklabels(all_metrics['Model'], rotation=45, ha='right')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# ROC-AUC (if available)\n",
        "roc_data = all_metrics[all_metrics['ROC-AUC'].notna()]\n",
        "if len(roc_data) > 0:\n",
        "    axes[1, 1].bar(roc_data['Model'], roc_data['ROC-AUC'], color='#2ecc71')\n",
        "    axes[1, 1].set_ylabel('ROC-AUC')\n",
        "    axes[1, 1].set_title('ROC-AUC Comparison')\n",
        "    axes[1, 1].set_ylim(0, 1)\n",
        "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('comprehensive_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Chart saved as 'comprehensive_comparison.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"FINAL EVALUATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "improvement_over_majority = ml_metrics['Accuracy'] - majority_metrics['Accuracy']\n",
        "improvement_over_random = ml_metrics['Accuracy'] - random_metrics['Accuracy']\n",
        "\n",
        "print(f\"\\n‚úÖ Model Performance:\")\n",
        "print(f\"   ML Model Accuracy: {ml_metrics['Accuracy']:.4f} ({ml_metrics['Accuracy']*100:.2f}%)\")\n",
        "print(f\"   Improvement over Majority Class: +{improvement_over_majority:.4f} ({improvement_over_majority*100:+.2f}%)\")\n",
        "print(f\"   Improvement over Random: +{improvement_over_random:.4f} ({improvement_over_random*100:+.2f}%)\")\n",
        "print(f\"   ROC-AUC: {ml_metrics['ROC-AUC']:.4f}\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  Areas for Improvement:\")\n",
        "if ml_metrics['Improving_F1'] < 0.4:\n",
        "    print(f\"   - 'Improving' class F1 is low ({ml_metrics['Improving_F1']:.4f})\")\n",
        "    print(f\"     Consider class balancing techniques\")\n",
        "if ml_metrics['Macro_F1'] < 0.6:\n",
        "    print(f\"   - Overall macro F1 could be improved ({ml_metrics['Macro_F1']:.4f})\")\n",
        "\n",
        "print(f\"\\nüìä Key Strengths:\")\n",
        "print(f\"   - Model beats baselines by meaningful margin\")\n",
        "print(f\"   - Good performance on 'Stable' class (F1: {ml_metrics['Stable_F1']:.4f})\")\n",
        "print(f\"   - Moderate performance on 'Declining' class (F1: {ml_metrics['Declining_F1']:.4f})\")\n",
        "\n",
        "print(f\"\\nüéØ Recommendations:\")\n",
        "print(f\"   1. Address class imbalance for 'Improving' class\")\n",
        "print(f\"   2. Consider feature selection to reduce complexity\")\n",
        "print(f\"   3. Test different hyperparameters\")\n",
        "print(f\"   4. Consider ensemble methods\")\n",
        "print(f\"   5. Validate temporal structure to ensure no data leakage\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
