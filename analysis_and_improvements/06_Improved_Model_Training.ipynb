{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 06. Improved Model Training\n",
        "\n",
        "## Objective\n",
        "Train improved models incorporating:\n",
        "1. Best class balancing technique (from analysis)\n",
        "2. Feature selection based on importance\n",
        "3. Optimized hyperparameters\n",
        "4. Comparison with original model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import (accuracy_score, classification_report, \n",
        "                           confusion_matrix, roc_auc_score)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "\n",
        "print(\"‚úÖ Libraries imported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data and Prepare Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = pd.read_csv('../today/trajectory_ml_ready_advanced.csv')\n",
        "\n",
        "# Prepare features and target\n",
        "drop_cols = ['UNITID', 'Institution_Name', 'Year', 'Target_Trajectory', 'Target_Label', 'State']\n",
        "X = df.drop(columns=drop_cols)\n",
        "y = df['Target_Label'].astype(int)\n",
        "\n",
        "# Split data (same as original)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training Set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
        "print(f\"Test Set: {X_test.shape[0]} samples\")\n",
        "print(f\"\\nClass Distribution (Train):\")\n",
        "print(y_train.value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Selection Based on Importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load original model to get feature importance\n",
        "original_model = joblib.load('../today/models/final_trajectory_model.joblib')\n",
        "xgb_original = original_model.named_steps['classifier']\n",
        "preprocessor_original = original_model.named_steps['preprocessor']\n",
        "\n",
        "# Get feature names and importance\n",
        "feature_names = preprocessor_original.get_feature_names_out()\n",
        "importances = xgb_original.feature_importances_\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "# Select top features (95% cumulative importance)\n",
        "cumulative_importance = feature_importance_df['Importance'].cumsum()\n",
        "total_importance = feature_importance_df['Importance'].sum()\n",
        "threshold_95 = 0.95\n",
        "n_features_95 = (cumulative_importance / total_importance <= threshold_95).sum() + 1\n",
        "\n",
        "top_features = feature_importance_df.head(n_features_95)['Feature'].tolist()\n",
        "\n",
        "print(f\"Original Features: {len(feature_names)}\")\n",
        "print(f\"Selected Features (95% importance): {n_features_95}\")\n",
        "print(f\"\\nTop 10 Selected Features:\")\n",
        "for i, feat in enumerate(top_features[:10], 1):\n",
        "    print(f\"  {i}. {feat}\")\n",
        "\n",
        "# Note: We'll use all features for now, but can filter later if needed\n",
        "# The preprocessor will handle feature selection during training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build Improved Pipeline with Best Balancing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Based on class imbalance analysis, use SMOTE (typically best for this problem)\n",
        "# Build preprocessing pipeline\n",
        "categorical_cols = ['Division']\n",
        "numerical_cols = [col for col in X.columns if col not in categorical_cols]\n",
        "\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', categorical_transformer, categorical_cols),\n",
        "        ('num', 'passthrough', numerical_cols)\n",
        "    ])\n",
        "\n",
        "# Improved hyperparameters (from original tuning + adjustments)\n",
        "best_params = {\n",
        "    'n_estimators': 300,\n",
        "    'max_depth': 3,\n",
        "    'learning_rate': 0.05,\n",
        "    'subsample': 0.7,\n",
        "    'colsample_bytree': 0.9,\n",
        "    'min_child_weight': 3,\n",
        "    'eval_metric': 'mlogloss',\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Build pipeline with SMOTE\n",
        "improved_pipeline = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', XGBClassifier(**best_params))\n",
        "])\n",
        "\n",
        "print(\"‚úÖ Improved pipeline created with:\")\n",
        "print(\"   - SMOTE for class balancing\")\n",
        "print(\"   - Optimized hyperparameters\")\n",
        "print(\"   - Proper preprocessing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Improved Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the improved model\n",
        "print(\"Training improved model...\")\n",
        "improved_pipeline.fit(X_train, y_train)\n",
        "print(\"‚úÖ Model trained successfully\")\n",
        "\n",
        "# Get predictions\n",
        "y_pred_improved = improved_pipeline.predict(X_test)\n",
        "y_prob_improved = improved_pipeline.predict_proba(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy_improved = accuracy_score(y_test, y_pred_improved)\n",
        "roc_auc_improved = roc_auc_score(y_test, y_prob_improved, multi_class='ovr')\n",
        "\n",
        "print(f\"\\nImproved Model Performance:\")\n",
        "print(f\"  Accuracy: {accuracy_improved:.4f} ({accuracy_improved*100:.2f}%)\")\n",
        "print(f\"  ROC-AUC: {roc_auc_improved:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_improved, \n",
        "                          target_names=['Declining', 'Stable', 'Improving']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compare with Original Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get original model predictions\n",
        "y_pred_original = original_model.predict(X_test)\n",
        "y_prob_original = original_model.predict_proba(X_test)\n",
        "\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "roc_auc_original = roc_auc_score(y_test, y_prob_original, multi_class='ovr')\n",
        "\n",
        "# Calculate per-class metrics for both\n",
        "def get_class_metrics(y_true, y_pred, class_name):\n",
        "    report = classification_report(y_true, y_pred, \n",
        "                                 target_names=['Declining', 'Stable', 'Improving'],\n",
        "                                 output_dict=True)\n",
        "    return report[class_name]\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Model': ['Original', 'Improved'],\n",
        "    'Accuracy': [accuracy_original, accuracy_improved],\n",
        "    'ROC-AUC': [roc_auc_original, roc_auc_improved],\n",
        "    'Declining_F1': [\n",
        "        get_class_metrics(y_test, y_pred_original, 'Declining')['f1-score'],\n",
        "        get_class_metrics(y_test, y_pred_improved, 'Declining')['f1-score']\n",
        "    ],\n",
        "    'Stable_F1': [\n",
        "        get_class_metrics(y_test, y_pred_original, 'Stable')['f1-score'],\n",
        "        get_class_metrics(y_test, y_pred_improved, 'Stable')['f1-score']\n",
        "    ],\n",
        "    'Improving_F1': [\n",
        "        get_class_metrics(y_test, y_pred_original, 'Improving')['f1-score'],\n",
        "        get_class_metrics(y_test, y_pred_improved, 'Improving')['f1-score']\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(comparison.to_string(index=False))\n",
        "\n",
        "# Calculate improvements\n",
        "improvement_acc = accuracy_improved - accuracy_original\n",
        "improvement_auc = roc_auc_improved - roc_auc_original\n",
        "\n",
        "print(f\"\\nüìà Improvements:\")\n",
        "print(f\"   Accuracy: {improvement_acc:+.4f} ({improvement_acc*100:+.2f}%)\")\n",
        "print(f\"   ROC-AUC: {improvement_auc:+.4f} ({improvement_auc*100:+.2f}%)\")\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Accuracy\n",
        "axes[0, 0].bar(comparison['Model'], comparison['Accuracy'], color=['#3498db', '#2ecc71'])\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].set_title('Accuracy Comparison')\n",
        "axes[0, 0].set_ylim(0, 1)\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# ROC-AUC\n",
        "axes[0, 1].bar(comparison['Model'], comparison['ROC-AUC'], color=['#3498db', '#2ecc71'])\n",
        "axes[0, 1].set_ylabel('ROC-AUC')\n",
        "axes[0, 1].set_title('ROC-AUC Comparison')\n",
        "axes[0, 1].set_ylim(0, 1)\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Per-class F1\n",
        "x = np.arange(len(comparison))\n",
        "width = 0.25\n",
        "axes[1, 0].bar(x - width, comparison['Declining_F1'], width, label='Declining', color='#e74c3c')\n",
        "axes[1, 0].bar(x, comparison['Stable_F1'], width, label='Stable', color='#3498db')\n",
        "axes[1, 0].bar(x + width, comparison['Improving_F1'], width, label='Improving', color='#2ecc71')\n",
        "axes[1, 0].set_ylabel('F1-Score')\n",
        "axes[1, 0].set_title('Per-Class F1-Score')\n",
        "axes[1, 0].set_xticks(x)\n",
        "axes[1, 0].set_xticklabels(comparison['Model'])\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Confusion matrices side by side\n",
        "cm_original = confusion_matrix(y_test, y_pred_original)\n",
        "cm_improved = confusion_matrix(y_test, y_pred_improved)\n",
        "\n",
        "sns.heatmap(cm_original, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1],\n",
        "            xticklabels=['Declining', 'Stable', 'Improving'],\n",
        "            yticklabels=['Declining', 'Stable', 'Improving'])\n",
        "axes[1, 1].set_title('Confusion Matrix - Original')\n",
        "axes[1, 1].set_ylabel('True Label')\n",
        "axes[1, 1].set_xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Chart saved as 'model_comparison.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Improved Model (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save improved model if it performs better\n",
        "if accuracy_improved > accuracy_original or roc_auc_improved > roc_auc_original:\n",
        "    model_filename = 'improved_trajectory_model.joblib'\n",
        "    joblib.dump(improved_pipeline, model_filename)\n",
        "    print(f\"‚úÖ Improved model saved to {model_filename}\")\n",
        "    print(f\"   (Better than original: Accuracy {improvement_acc:+.4f}, ROC-AUC {improvement_auc:+.4f})\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Improved model did not outperform original\")\n",
        "    print(\"   Consider different hyperparameters or balancing techniques\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
